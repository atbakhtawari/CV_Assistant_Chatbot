bapuji sr hadoop developer phone 1 224 7060020 email bapujidbjgmailcom professional summary 8 year experience including 4 year big data ecosystem related technology full project development implementation deployment strong experience working various hadoop ecosystem component like map reduce hdfs hive sqoop pig flume oozie strong knowledge architecture distributed system parallel processing framework indepth understanding mapreduce framework spark execution model worked extensively fine tuning long running spark application utilize better parallelism executor memory caching strong experience working batch realtime processing using spark framework expertise developing production ready spark application utilizing spark core data frame spark sql spark ml spark streaming apis hand experience installing configuring deploying hadoop distribution cloud environment amazon web service expertise developing production ready spark application utilizing spark core data frame spark sql spark ml spark streaming apis worked building real time data workflow using kafka spark streaming hbase worked extensively hive building complex data analytical application good understanding partition bucketing concept hive designed managed external table hive optimize performance used custom ser de like regex serde json serde csv serde etc hive handle mu ltiple format data knowledge apache ambari platform securing managing monitoring hadoop cluster experienced cluster coordination service zookeeper strong experience using different columnar file format like avro rcfile orc parquet format worked sqoop move importexport data relational database hadoop experience working hadoop cluster using cloudera amazon emr hortonworks distribution extensive experience performing etl structured semi structured data using pig latin script designed implemented hive pig udfs using java evaluation filtering loading storing data experienced job workflow scheduling monitoring tool like oozie well versed unix linux command line shell script adequate knowledge working experience agile methodology technical skill language java scala sql plsql pig latin python hive ql web technology jee jdbc jsp servlet jsf jstl ajax javascript big data system hadoop hdfs mapreduce yarn pig hive sqoop flume oozie impala spark apache airflow kafka splunk cloudera cdh4 cdh5 horton work hadoop emr talend ranger rdbms oracle 10g11g mysql sql server 20052008 r2 postgresql db2 teradata nosql database hbase mongodb cassandra appweb server apache tomcat weblogic soa web service soap rest framework strut 2 hibernate spring 3x version control git cv svn ides eclipse scala ide netbeans intellij idea operating system unix linux window education bachelor technology computer science engineering jntu kakinada andhra pradesh india work experience cigna bloomfield connecticut jul17 present role hadoop spark developer responsibility developed spark application using scala utilizing data frame spark sql api faster processing data developed highly optimized spark application perform various data cleansing validation transformation summarization activity according requirement data pipeline consists spark hive sqoop custom built input adapter ingest transform analyze operational data developed spark job hive job summarize transform data used spark interactive query processing streaming data integration nosql database hbase cassandra interactive access pattern involved converting hive query spark transformation using spark data frame scala automated creation termination aws emr cluster using aws java sdk built real time data pipeline developing kafka producer spark streaming application consuming ingested syslog message kafka worked apache airflow schedule single sometimes complex chain task depend regular interval handled importing data relational database hdfs using sqoop performing transformation using hive spark knowledge apache ambari platform securing managing monitoring hadoop cluster exported processed data relational database using sqoop visualize generate report bi team experienced cluster coordination service zookeeper installed tested deployed monitoring solution splunk service used hive analyze partitioned bucketed data compute various metric reporting developed hive script hive ql de normalize aggregate data scheduled executed workflow oozie run various job designing creating etl job talend load huge volume data cassandra hadoop ecosystem relational database environment hadoop spark hive java scala maven impala oozie oracle ambari github tableau unix hortonworks apache airflow kafka zookeeper sqoop cassandra talend splunk hbase qualcomm san diego ca dec16 jun17 role hadoop spark developer responsibility part big data center excellence coe responsible designing building enterprise data analytics platform worked respective business unit understanding scope analytics requirement performed core etl transformation spark automated data pipeline involve data ingestion data cleansing data preparation data analytics created end end spark application using scala perform various data cleansing validation transformation summarization activity user behavioral data developed end toend data pipeline using ftp adaptor spark hive impala implemented spark utilizing spark sql heavily faster development processing data exploring spark improving performance optimization existing job hadoop using sparksql data frame running yarn mode handled importing enterprise data different data source hdfs using sqoop performing transformation using hive map reduce loading data hbase table collecting aggregating large amount log data using flume staging data hdfs analysis wrapper developed python instantiating multithreaded application running application analyzed data performing hive query hive ql running pig script pig latin study customer behavior data warehousing experience design development testing implementation support enterprise data warehouse used hive analyze partitioned bucketed data compute various metric reporting created component like hive udfs missing functionality hive analytics worked various performance optimization like using distributed cache small datasets partition bucketing hive map side join created oozie workflow coordinator automate data pipeline daily weekly monthly automated creation termination aws emr cluster using aws java sdk environment aws emr hadoop spark hive sqoop hbase unix talend pig linux java scala python ambari zookeeper hortonworks mckesson alpharetta ga dec15 nov16 hadoop spark developer responsibility developed multithreaded java based input adaptor ingesting click stream data external source like ftp server s3 bucket daily basis created various spark application using scala perform various enrichment click stream data combined enterprise data user implemented batch processing job using spark scala api developed sqoop script importexport data oracle hdfs hive table stored data columnar format using hive involved building managing nosql database model using hbase worked spark read data hive write hbase optimized hive table using optimization technique like partition bucketing provide better performance hive ql query worked multiple file format like avro sequence parquet orc converted existing mapreduce program spark application handling semi structured data like json file apache log file custom log data loaded final processed data hbase table allow downstream application team build rich data driven application worked team improve performance optimization existing algorithm hadoop using spark spark sql data frame implemented business logic hive written udfs process data analysis used oozie define workflow coordinate execution spark hive sqoop job addressing issue occurring due huge volume data transition designed documented operational problem following standard procedure using jira environment java hadoop 210 map reduce2 spark unix pig 0120 hive 0130 linux sqoop 142 flume 131 eclipse aws ec2 cloudera cdh 4 american home shield memphis tn dec14 nov15 role hadoop develope r responsibility migrated needed data mysql hdfs using sqoop importing various format flat file hdfs mainly worked hive query categorize data different claim involved loading data linux file system hdfs written customized hive udfs java functionality complex implemented partitioning dynamic partition bucket hive designing creating hive external table using shared meta store instead derby partitioning dynamic partitioning bucket generate final reporting data using tableau testing connecting corresponding hive table using hive odbc connector responsible manage test data coming different source reviewing peer table creation hive data loading query weekly meeting technical collaborator active participation code review session senior junior developer monitored system health log respond accordingly warning failure condition gained experience managing reviewing hadoop log file involved scheduling oozie workflow engine run multiple hive pig job involved unit testing interface testing system testing user acceptance testing workflow tool created maintained technical documentation launching hadoop cluster executing hive query pig script environment apache hadoop hdfs hive map reduce core java pig sqoop cloudera cdh4 oracle mysql protective life edina mn oct13 nov14 role java developer responsibility implemented web based application using servlets jsp spring jdbc xml involved writing spring configuration xml file contains declaration dependent object declaration used hibernate connect database create dao layer developed application framework using model view controller using technology spring used html xhtml xml xslt xpath jsp tag library develop view page multilayer application construction using open jpa html5 spring mvc annotated spring architecture spring bean implemented unix shell script migrate various data file sp rating repository implemented smooth pagination capability using jsp remove existing pagination utility worked geo api provide geological access capability spcom site involved agile process streamline development process iterative development code review managing cv repository prepare build dev uat environment participating regular team meeting sprint planning meeting user story review meeting etc involved preparing high low level design doc uml diagram using microsoft visio tool environment jdk 15 xml html xhtml jsp spring dao oracle express edition apache ant cv junit unix log4j cs style sheet apache tomcat j2ee maven 3 accenture hyderabad india oct11 sep13 role java developer responsibility involved requirement analysis design development testing involved setting different role maintained authentication application designed deployed tested multi tier application using java technology involved front end development using jsp html cs implemented application using servlets deployed application oracle web logic server implemented multithreading concept java class avoid deadlocking used mysql database store data execute sql query backend prepared maintained test environment tested application going live production documented communicated test result team lead daily basis involved weekly meeting team lead manager discus issue status project environment j2ee java jsp jdbc multi threading html oracle web logic server eclipse mysql junit golan technology hyderabad india jun09 sep11 role java developer golan technology range turnkey solution custom client driven solution variety product category including website development platform based application demand intelligence business insight generation smart site ability provide unified user experience consistent messaging website across globe driving favorable brand impression responsibility involved analysis design implementation testing project developed ui using html javascript cs jsp interactive cross browser functionality complex user interface implemented end toend functionality client requirement development phase implemented functionality mapping entity database using hibernate written sql query involved jdbc connection accordance business logic performed various level unit testing entire application using test case included preparation detail documentation result actively participated client meeting taking input additional functionality involved fixing bug unit testing test case using junit environment j2ee spring hibernate javascript cs servlets mysql